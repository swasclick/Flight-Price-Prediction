# âœˆ Predicting Flight Fares with ML: A Deep Dive from Chaos to Clarity ðŸš€

Just completed a hands-on Flight Fare Prediction project that went beyond the basics â€” not just another regression model, but a data science journey filled with learning, iteration, and surprising results.

---

## ðŸ” What I Did:

### ðŸ§¹ Smart Missing Value Imputation:
Instead of dropping rows or using simple mean/mode, I leveraged **Random Forest Classifier** and **KNN-based imputation** which included **label encoding** the categorical data to predict missing values intelligently, preserving valuable patterns in the data.

---

### ðŸ§  Insightful Feature Engineering:

- Clipped outliers based on **IQR-based bounds**  
- Used **one-hot encoding** for features with fewer classes, and **target encoder** for features with large class numbers

---

## ðŸ§ª Model Building & Results:

ðŸ§  **11 different Models** â€” From Linear Regression to Neural Network Regressors, battled it out to crush **Mean Squared Errors**, but the takeaway?

ðŸ‘‰ **More complex â‰  better predictions**.  
> - I tried to **log transform** a few numerical columns that were right-skewed, but the models made way worse predictions, because they overfit the clumped-up data points! Simply **clipping the (very few) outliers** boosted the performance several fold.  
> - Despite tuning, **Neural Networks were outperformed by simpler ensemble models**.

---

### ðŸ¥‡ In the end, the good ol' **Random Forest Regressor** dominated the leaderboard â€” delivering:

âœ… **Max Baseline RÂ²**: `97.8%`  
âœ… **Tuned, Cross-Validated, Generalized RÂ²**: `97.6%`

---

## ðŸ“Œ Key Takeaways:

- **EDA + Feature Engineering = 70% of the work that drives results**  
- **Simpler models can outperform deep ones â€” especially on structured tabular data**  
- **Ensemble models like Random Forest can be both powerful and interpretable**
